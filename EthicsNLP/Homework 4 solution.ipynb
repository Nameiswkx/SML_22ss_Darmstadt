{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjyW3pQzqUYD"
      },
      "source": [
        "# Ethics for NLP: Spring 2022\n",
        "# Homework 4 Privacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbK8L5kJJyvX"
      },
      "source": [
        "## 1. Data Overview and Baseline\n",
        "\n",
        "A major problem with utilizing web data as a source for NLP applications is the increasing concern for privacy, e.g., such as microtargeting. This homework is aimed at developing a method to obfuscate demographic features, in this case (binary) gender and to investigate the trade-off between obfuscating an users identity and preserving useful information.\n",
        "\n",
        "The given dataset consists of Reddit posts (`post_text`) which are annotated with the gender (`op_gender`) of the user and the corresponding subreddit (`subreddit`) category.\n",
        "\n",
        "*  `subreddit_classifier.pickle` pretrained subreddit classifier\n",
        "*  `gender_classifier.pickle` pretrained gender classifier\n",
        "*  `test.csv` your primary test data\n",
        "*  `male.txt` a list of words commonly used by men\n",
        "*  `female.txt` a list of words commonly used by women\n",
        "*  `background.csv` additional Reddit posts that you may optionally use for training an obfuscation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42JI8l18r39p"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.corpora import Dictionary\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import cloudpickle\n",
        "import pandas\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5oiaDj5QNq9"
      },
      "outputs": [],
      "source": [
        "def get_preds(cache_name: str, test: List[str]) -> List[str]:\n",
        "    loaded_model, dictionary, transpose, train_bow = pickle.load(open(cache_name, 'rb'))\n",
        "    X_test = transpose(test, train_bow, dictionary)\n",
        "    preds = loaded_model.predict(X_test)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoT-ItJ40d1k"
      },
      "outputs": [],
      "source": [
        "def run_classifier(test_file: str) -> Tuple[float]:\n",
        "    test_data = pandas.read_csv(test_file)\n",
        "\n",
        "    cache_name = 'gender_classifier.pickle'\n",
        "    test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n",
        "    gold_test = list(test_data[\"op_gender\"])\n",
        "    gender_acc = accuracy_score(list(test_preds), gold_test)\n",
        "    print(\"Gender classification accuracy\", gender_acc)\n",
        "\n",
        "    cache_name = 'subreddit_classifier.pickle'\n",
        "    test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n",
        "    gold_test = list(test_data[\"subreddit\"])\n",
        "    subreddit_acc = accuracy_score(list(test_preds), gold_test)\n",
        "    print(\"Subreddit classification accuracy\", subreddit_acc)\n",
        "    return gender_acc, subreddit_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf7nYEb0QPtU"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(\"test.csv\")\n",
        "\n",
        "assert gender_acc == 0.646\n",
        "assert subreddit_acc == 0.832"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LKaoI6TZJpt"
      },
      "source": [
        "**Default accuracy:**\n",
        "*   `Gender    classification accuracy: 0.646`\n",
        "*   `Subreddit classification accuracy: 0.832`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zl4YplkJgcQ"
      },
      "source": [
        "## 2. Obfuscation of the Test Dataset\n",
        "### 2.1 Random Obfuscated Dataset  (4P)\n",
        "First, run a random experiment, by randomly swapping gender-specific words that appear in posts with a word from the respective list of words of the opposite gender.\n",
        "\n",
        "*  Write a function to read the female.txt and male.txt files\n",
        "*  Tokenize the posts (‚Äûpost_text‚Äú) using NLTK (0.5p)\n",
        "*  For each post, if written by a man (‚ÄûM‚Äú) and containing a token from the male.txt, replace that token with a random one from the female.txt (1p)\n",
        "*  For each post, if written by a woman (‚ÄûW‚Äú) and containing a token from the female.txt, replace that token with a random one from the male.txt (1p)\n",
        "*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Run the given classifier again, report the accuracy and provide a brief commentary on the results compared to the baseline (1p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWgbsU1bwkBQ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Solution\n",
        "#\n",
        "def read_data(file_name: str) -> List[str]:\n",
        "    with open(file_name) as file: return [l.replace(\"\\n\", \"\") for l in file.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAcIf9Ck521o"
      },
      "outputs": [],
      "source": [
        "male_words = read_data(\"./male.txt\")\n",
        "female_words = read_data(\"./female.txt\")\n",
        "\n",
        "assert len(male_words) == 3000\n",
        "assert len(male_words) == 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZUdjzW3x375"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Solution\n",
        "#\n",
        "def random_replace(x: str, words_1: List[str], words_2: List[str]) -> str:\n",
        "  return \" \".join(random.choice(words_1) if t.lower() in words_2 else t for t in nltk.tokenize.word_tokenize(x))\n",
        "\n",
        "def obfuscate_gender(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n",
        "  data = pandas.read_csv(dataset_file_name)\n",
        "  data.loc[data['op_gender'] == \"M\", 'post_text'] = data.loc[data['op_gender'] == \"M\", 'post_text'].apply(lambda x: random_replace(x, female_words, male_words))\n",
        "  data.loc[data['op_gender'] == \"W\", 'post_text'] = data.loc[data['op_gender'] == \"W\", 'post_text'].apply(lambda x: random_replace(x, male_words, female_words))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzXQAIxJg9Hn"
      },
      "outputs": [],
      "source": [
        "file_name = \"random_replaced_test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLnNKpwwtqo"
      },
      "outputs": [],
      "source": [
        "random_replaced_test = obfuscate_gender(male_words=male_words, female_words=female_words, dataset_file_name=\"test.csv\")\n",
        "random_replaced_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNiDfkgPm9DY"
      },
      "outputs": [],
      "source": [
        "random_replaced_test = pandas.read_csv(file_name)\n",
        "assert len(random_replaced_test) == 500\n",
        "assert random_replaced_test[\"subreddit\"][0] == \"funny\"\n",
        "assert random_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osyy2rw9J8X1"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9AsCLRtYXc8"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: `\n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OHsp4B7Jqnv"
      },
      "source": [
        "### 2.2 Similarity Obfuscated Dataset (4P)\n",
        "In a second approach, refine the swap method. Instead of randomly selecting a word, use a similarity metric.\n",
        "\n",
        "\n",
        "*  Instead of the first method replace the tokens by semantically similar tokens from the other genders token list. For that you may choose any metric for identifying semantically similar words, but you have to justify your choice. (Recommend: using cosine distance between pre-trained word embeddings) (2p)\n",
        "*  Save the obfuscated version of the test.csv in a separate CSV file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Run the given classifier again, report the accuracy and provide a brief commentary on the results (compared to the baseline and your other results) (1p)\n",
        "*  The classifiers accuracy for predicting the gender should be below random guessing (50%) and for the subreddit prediction it should be above 80% (0.5p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYAS5eXqcEoe"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "model = gensim.downloader.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wmJek6JcIf_"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Solution\n",
        "#\n",
        "def similar(a: str, b: str) -> float:\n",
        "  try:\n",
        "    return model.similarity(a, b)\n",
        "  except:\n",
        "    return 0\n",
        "\n",
        "def max_similar(token: str, words: List[str]) -> str:\n",
        "  return max([(i, similar(i, token)) for i in words], key=lambda x:x[1])[0]\n",
        "\n",
        "def similarity_replace(x: str, words_1: List[str], words_2: List[str]) -> str:\n",
        "  return \" \".join(max_similar(t, words_1) if t.lower() in words_2 else t for t in nltk.tokenize.word_tokenize(x))\n",
        "\n",
        "def obfuscate_gender(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n",
        "  data = pandas.read_csv(dataset_file_name)\n",
        "  data.loc[data['op_gender'] == \"M\", 'post_text'] = data.loc[data['op_gender'] == \"M\", 'post_text'].apply(lambda x: similarity_replace(x, female_words, male_words))\n",
        "  data.loc[data['op_gender'] == \"W\", 'post_text'] = data.loc[data['op_gender'] == \"W\", 'post_text'].apply(lambda x: similarity_replace(x, male_words, female_words))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo1vZ0aZhHb6"
      },
      "outputs": [],
      "source": [
        "file_name = \"similarity_replaced_test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPcGRD06UFN"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = obfuscate_gender(male_words=male_words, female_words=female_words, dataset_file_name=\"./test.csv\")\n",
        "similarity_replaced_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl6yL2mTmDRX"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = pandas.read_csv(file_name)\n",
        "assert len(similarity_replaced_test) == 500\n",
        "assert similarity_replaced_test[\"subreddit\"][0] == \"funny\"\n",
        "assert similarity_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAgAPPSrLWsK"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gapuF5CTZCK-"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkdkD89GoxDo"
      },
      "source": [
        "### 2.3 Your Own Obfuscated Dataset (4P)\n",
        "With this last approach, you can experiment by yourself how to obfuscate the posts.\n",
        "\n",
        "*  Some examples: What if you randomly decide whether or not to replace words instead of replacing every lexicon word? What if you only replace words that have semantically similar enough counterparts? What if you use different word embeddings? (2p)\n",
        "*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Describe your modifications and report the accuracy and provide a brief commentary on the results compared to the baseline and your other results (1.5p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY8hfLFN3B4a"
      },
      "outputs": [],
      "source": [
        "model = gensim.downloader.load(\"glove-twitter-200\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqHeE7_3rpje"
      },
      "outputs": [],
      "source": [
        "file_name = \"similarity_glove_replaced_test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr_TZnYFrsKc"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = obfuscate_gender(male_words=male_words, female_words=female_words, dataset_file_name=\"./test.csv\")\n",
        "similarity_replaced_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK55G3jAruJ6"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = pandas.read_csv(file_name)\n",
        "assert len(similarity_replaced_test) == 500\n",
        "assert similarity_replaced_test[\"subreddit\"][0] == \"funny\"\n",
        "assert similarity_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGvYorhrwBx"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US3Gcok5qdYo"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48j9qFLJFygB"
      },
      "source": [
        "### 3 Advanced Obfuscated Model (5P)\n",
        "Develop your own obfuscation model using the provided background.csv for training. Your ultimate goal should be to obfuscate text so that the classifier is unable to determine the gender of an user (no better than random guessing) without compromising the accuracy of the subreddit classification task. To train a model that is good at predicting subreddit classification, but bad at predicting gender. The key idea in this approach is to design a model that does not encode information about protected attributes (in this case, gender). In your report, include a description of your model and results.\n",
        "\n",
        "*  Develop your own classifier (3p)\n",
        "*  Use only posts from the subreddits ‚ÄûCasualConversation‚Äú and ‚Äûfunny‚Äú (min. 1000 posts for each gender per subreddit) (0.5p)\n",
        "*  Use sklearn models (MLPClassifier, LogisticRegression, etc.)\n",
        "*  Use 90% for training and 10% for testing (0.5p)\n",
        "*  In your report, include a description of your model and report the accuracy on the unmodified train data (your baseline here) as well as the modified train data and provide a brief commentary on the results (1p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLOBmI2AOxqk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_data(df_, labels, max_):\n",
        "  df = [df_.loc[(df_['subreddit'] == l) & (df_['op_gender'] == \"W\"), ['post_text', 'op_gender', 'subreddit']].head(max_) for l in labels]\n",
        "  df += [df_.loc[(df_['subreddit'] == l) & (df_['op_gender'] == \"M\"), ['post_text', 'op_gender', 'subreddit']].head(max_)for l in labels]\n",
        "  return pandas.concat(df)"
      ],
      "metadata": {
        "id": "XBfjZNFzo1Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AotLon2kgo5b"
      },
      "outputs": [],
      "source": [
        "train_data = pandas.read_csv(\"background.csv\")\n",
        "labels = [\"CasualConversation\", \"funny\"]\n",
        "train_data = get_train_data(train_data, labels, 4000)\n",
        "train_data_original = train_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "kPCktKXFy2Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32TVMhTlMUAs"
      },
      "outputs": [],
      "source": [
        "train_data = shuffle(train_data)\n",
        "test_data = train_data.head(round(len(train_data) * 0.1))\n",
        "train_data = train_data[round(len(train_data) * 0.1):]\n",
        "print(len(test_data))\n",
        "print(len(train_data))\n",
        "\n",
        "train_data.to_csv(\"mode_train_data.csv\")\n",
        "\n",
        "def simple_modify(data):\n",
        "  copy_1 = data.copy()\n",
        "  copy_2 = data.copy()\n",
        "  copy_1.loc[copy_1['op_gender'] == \"M\", 'op_gender'] = \"W\"\n",
        "  copy_2.loc[copy_2['op_gender'] == \"W\", 'op_gender'] = \"M\"\n",
        "  new_data = pandas.concat([copy_1, copy_2])\n",
        "  print(len(new_data))\n",
        "  new_data.to_csv(\"mode_train_data_modified.csv\")\n",
        "  return new_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_modified = simple_modify(train_data)"
      ],
      "metadata": {
        "id": "iqIWd9YppPko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSnwtMEOUlY5"
      },
      "outputs": [],
      "source": [
        "def embedd_data(train_data, test_data):\n",
        "  # bag-of-words representation\n",
        "  train = [[token.lower() for token in nltk.tokenize.word_tokenize(t) if token not in nltk.corpus.stopwords.words('english')] for t in train_data[\"post_text\"]]\n",
        "  test = [[token.lower() for token in nltk.tokenize.word_tokenize(t) if token not in nltk.corpus.stopwords.words('english')] for t in test_data[\"post_text\"]]\n",
        "  dictionary = Dictionary(train + test)\n",
        "  dictionary.filter_extremes(no_below=5)\n",
        "  len_train = len(train)\n",
        "  all_feats = gensim.matutils.corpus2csc([dictionary.doc2bow(x) for x in train + test]).transpose()\n",
        "  test_feats = all_feats[len_train:]\n",
        "  train_feats = all_feats[:len_train]\n",
        "  return dictionary, train_feats, test_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP-wCAdgA2il"
      },
      "outputs": [],
      "source": [
        "def classify(train_data, test_data):\n",
        "  dictionary, train_feats, test_feats = embedd_data(train_data, test_data)\n",
        "\n",
        "  train_y = list(train_data[\"op_gender\"])\n",
        "  test_y = list(test_data[\"op_gender\"])\n",
        "  model_ = LogisticRegression(max_iter=10000)\n",
        "  model_.fit(train_feats, train_y)\n",
        "  predicted = model_.predict(test_feats)\n",
        "  print(\"gender prediction:\", np.mean(predicted == test_y))\n",
        "\n",
        "  train_y = list(train_data[\"subreddit\"])\n",
        "  test_y = list(test_data[\"subreddit\"])\n",
        "  grid = {\n",
        "      \"penalty\": ['l2'],\n",
        "  }\n",
        "  logreg = LogisticRegression(max_iter=10000)\n",
        "  model_ = GridSearchCV(logreg, grid, cv=10, verbose=0)\n",
        "  model_.fit(train_feats, train_y)\n",
        "  predicted = model_.predict(test_feats)\n",
        "  print(\"subreddit prediction:\", np.mean(predicted == test_y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify(train_data_original, test_data)"
      ],
      "metadata": {
        "id": "5DFb18B_l9FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(train_data_modified, test_data)"
      ],
      "metadata": {
        "id": "C3tUBLEZmVD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoePMqHbZE9p"
      },
      "source": [
        "**Report accuracy:**\n",
        "* Baseline:\n",
        "  * `Gender    classification accuracy: `\n",
        "  * `Subreddit classification accuracy: `\n",
        "* Your Model: \n",
        "  * `Gender    classification accuracy: `\n",
        "  * `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZERIDpgwnj_"
      },
      "source": [
        "### 4 Ethical Implications (3P)\n",
        "Discuss the ethical implications of obfuscation and privacy based on the concepts covered in the lecture. Provide answers to the following points:\n",
        "\n",
        "1.   What are demographic features (name at least three) and explain shortly some of the privacy violation risks? (1p)\n",
        "2.   Explain the cultural and social implications and their effects? In this context discuss the information privacy paradox. You may refer to a recent example like the COVID-19 pandemic.  (1.5p)\n",
        "3.   Name a at least three privacy preserving countermeasures  (0.5p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q_Qbw0xw3W0"
      },
      "source": [
        "1. - Examples: Gender; Age; Location; Religion; Ethnicity; Social class; Diet; Personality type \n",
        "  - Risks: Humiliation, Abuse, Discrimination, Identify theft, financial/physical/psychological/reputational damage or threat to life\n",
        "\n",
        "---\n",
        "\n",
        "2. \n",
        "\n",
        "Culturally, there are differences in societies, which are mainly reflected in differences in governmental stringency or power distance. For example, countries with comparatively high power distance were found to be more likely to control pandemic numbers. Different social values, such as obedience, orientation, trust, and commitment to rules and authority, serve a common goal of safety for all and allow more stringent measures to be evaluated by the outcome. Consequently, in countries such as Taiwan and Singapore, stricter measures could be implemented, while in many European countries the measures led to a loss of trust in authority and protests. Societies that value individual freedom and choice showed a more positive growth rate of COVID-19 cases than societies that value cooperation and collective well-being.( https://link.springer.com/article/10.1057/s41267-021-00455-w#Sec8)\n",
        "These differences were particularly evident in the acceptance of quarantine and other national protective measures, such as the wearing of masks or Apps to control the spread of infection, which were enforced with varying degrees of strictness. Since privacy is assumed to be a fundamental freedom in many countries, such as Germany. This shows a conflict between individual and authority or risk and security or safey in this specific case. \n",
        "However, it is contradictory from a social perspective, since on the one hand data is voluntarily disclosed for many online applications and in social networks, which are usually used for profiling functions, and on the other hand little active effort is made to protect one's own data. The intentions and actions regarding privacy often do not coincide, which is commonly referred to as the information privacy paradox. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "3. \n",
        "  * Anonymization (ùëò-anonymity, ùëô-diversity, ùë°-closeness)\n",
        "  * Differential Privacy \n",
        "  * Encryption\n",
        "  * Privacy Aggregation of Teacher Ensenbles (PATE)\n",
        "  * Synthetic Data\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Homework_4_final_solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}