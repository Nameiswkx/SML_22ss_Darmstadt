{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92ac758-107f-468a-b59f-8ab5a0fa4d71",
   "metadata": {},
   "source": [
    "# Ethics for NLP: Spring 2022\n",
    "## Homework 3: Low Ressource Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad41e5",
   "metadata": {},
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8531a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements defined here\n",
    "# ----------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# set a fixed seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# hyperparameters\n",
    "params = {   \n",
    "    \"embedding_dim\": 100,\n",
    "    \"hidden_dim\":128,\n",
    "    \"n_layers\":2,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\":0.25,\n",
    "    \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863c71d",
   "metadata": {},
   "source": [
    "### BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9489a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: do not change anything in this code,\n",
    "# it can lead to incorrect results in the final accuracy calculation\n",
    "# ----------------\n",
    "\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_layers,\n",
    "        bidirectional,\n",
    "        dropout,\n",
    "        pad_idx,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # pass text through embedding layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # pass embeddings into LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # outputs holds the backward and forward hidden states in the final layer\n",
    "        # hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
    "\n",
    "        # we use our outputs to make a prediction of what the tag should be\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac97373",
   "metadata": {},
   "source": [
    "### Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ee0d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to train or evaluate model\n",
    "# depending on the input parameters\n",
    "\n",
    "def run(mode, lang, model_name):\n",
    "    print(\"Running model in {} mode with lang: {}\".format(mode, lang))\n",
    "    TEXT = data.Field(lower=True)\n",
    "    UD_TAGS = data.Field()\n",
    "\n",
    "    fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))\n",
    "\n",
    "    train_data, valid_data, test_data = datasets.UDPOS.splits(\n",
    "        fields=fields,\n",
    "        path=os.path.join(\"data\", lang),\n",
    "        train=\"{}-ud-train.conll\".format(lang),\n",
    "        validation=\"{}-ud-dev.conll\".format(lang),\n",
    "        test=\"{}-ud-test.conll\".format(lang),\n",
    "    )\n",
    "    MIN_FREQ = 2\n",
    "    print(os.path.join(\"data\", lang))\n",
    "\n",
    "    TEXT.build_vocab(train_data, min_freq=MIN_FREQ)\n",
    "    UD_TAGS.build_vocab(train_data)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "        print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "        print()\n",
    "        print(f\"Number of training examples: {len(train_data)}\")\n",
    "        print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "\n",
    "        print(f\"Number of tokens in the training set: {sum(TEXT.vocab.freqs.values())}\")\n",
    "\n",
    "    print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "    if mode == \"train\":\n",
    "        print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "        for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
    "            print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    model = BiLSTMPOSTagger(\n",
    "        input_dim=len(TEXT.vocab),\n",
    "        embedding_dim=params[\"embedding_dim\"],\n",
    "        hidden_dim=params[\"hidden_dim\"],\n",
    "        output_dim=len(UD_TAGS.vocab),\n",
    "        n_layers=params[\"n_layers\"],\n",
    "        bidirectional=params[\"bidirectional\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        pad_idx=PAD_IDX,\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "\n",
    "        def init_weights(m):\n",
    "            for name, param in m.named_parameters():\n",
    "                nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        model.apply(init_weights)\n",
    "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "        model.embedding.weight.data[PAD_IDX] = torch.zeros(params[\"embedding_dim\"])\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "    TAG_UNK_IDX = UD_TAGS.vocab.unk_index\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        N_EPOCHS = 10\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                model,\n",
    "                train_iterator,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                TAG_PAD_IDX,\n",
    "                TAG_UNK_IDX,\n",
    "            )\n",
    "            valid_loss, valid_acc = evaluate(\n",
    "                model, valid_iterator, criterion, TAG_PAD_IDX, TAG_UNK_IDX\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(\n",
    "                    model.state_dict(), \"saved_models/{}.pt\".format(model_name)\n",
    "                )\n",
    "\n",
    "            print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "            print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"saved_models/{}.pt\".format(model_name)))\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Model file `{}` doesn't exist. You need to train the model by running this code in train mode.\".format(\n",
    "                \"saved_models/{}.pt\".format(model_name)\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model, test_iterator, criterion, TAG_PAD_IDX, TAG_UNK_IDX\n",
    "    )\n",
    "    print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "def tag_percentage(tag_counts):\n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [\n",
    "        (tag, count, count / total_count) for tag, count in tag_counts\n",
    "    ]\n",
    "    return tag_counts_percentages\n",
    "\n",
    "\n",
    "def categorical_accuracy(preds, y, tag_pad_idx, tag_unk_idx):\n",
    "    max_preds = preds.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    )\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx) & (y != tag_unk_idx))\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.float().sum(), y[non_pad_elements].shape[0]\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, tag_pad_idx, tag_unk_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_n_label = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text)\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, tags)\n",
    "\n",
    "        correct, n_labels = categorical_accuracy(\n",
    "            predictions, tags, tag_pad_idx, tag_unk_idx\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct.item()\n",
    "        epoch_n_label += n_labels\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / epoch_n_label\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, tag_pad_idx, tag_unk_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_n_label = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            correct, n_labels = categorical_accuracy(\n",
    "                predictions, tags, tag_pad_idx, tag_unk_idx\n",
    "            )\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += correct.item()\n",
    "            epoch_n_label += n_labels\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / epoch_n_label\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2282e8c",
   "metadata": {},
   "source": [
    "### Task 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b3c92",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the english data (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50d7a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in eval mode with lang: en\n",
      "data\\en\n",
      "Number of testing examples: 2077\n",
      "Test Loss: 0.401 |  Test Acc: 91.66%\n"
     ]
    }
   ],
   "source": [
    "# english\n",
    "run(\"eval\", \"en\", \"en-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e9b4d",
   "metadata": {},
   "source": [
    "#### For the rest of this task, train and then evaluate the model on the trained data for the languages in each code cell (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d126933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: cs\n",
      "data\\cs\n",
      "Unique tokens in TEXT vocabulary: 38789\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 41559\n",
      "Number of validation examples: 9270\n",
      "Number of tokens in the training set: 719317\n",
      "Number of testing examples: 10148\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t175971\t\t24.5%\n",
      "PUNCT\t\t101318\t\t14.1%\n",
      "ADJ\t\t86855\t\t12.1%\n",
      "VERB\t\t79779\t\t11.1%\n",
      "ADP\t\t71491\t\t 9.9%\n",
      "PROPN\t\t46100\t\t 6.4%\n",
      "ADV\t\t37704\t\t 5.2%\n",
      "PRON\t\t34941\t\t 4.9%\n",
      "CONJ\t\t26714\t\t 3.7%\n",
      "NUM\t\t18004\t\t 2.5%\n",
      "SCONJ\t\t13186\t\t 1.8%\n",
      "DET\t\t12997\t\t 1.8%\n",
      "AUX\t\t9953\t\t 1.4%\n",
      "PART\t\t3825\t\t 0.5%\n",
      "SYM\t\t415\t\t 0.1%\n",
      "INTJ\t\t63\t\t 0.0%\n",
      "X\t\t1\t\t 0.0%\n",
      "The model has 4,514,567 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.836 | Train Acc: 72.50%\n",
      "\t Val. Loss: 0.282 |  Val. Acc: 91.89%\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.178 | Train Acc: 94.21%\n",
      "\t Val. Loss: 0.185 |  Val. Acc: 94.21%\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.113 | Train Acc: 96.28%\n",
      "\t Val. Loss: 0.165 |  Val. Acc: 94.73%\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.091 | Train Acc: 96.99%\n",
      "\t Val. Loss: 0.159 |  Val. Acc: 94.90%\n",
      "Epoch: 05 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.078 | Train Acc: 97.39%\n",
      "\t Val. Loss: 0.158 |  Val. Acc: 94.98%\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.067 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.162 |  Val. Acc: 95.03%\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.057 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.166 |  Val. Acc: 95.03%\n",
      "Epoch: 08 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.049 | Train Acc: 98.38%\n",
      "\t Val. Loss: 0.176 |  Val. Acc: 94.94%\n",
      "Epoch: 09 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.58%\n",
      "\t Val. Loss: 0.184 |  Val. Acc: 94.86%\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.192 |  Val. Acc: 94.87%\n",
      "Test Loss: 0.189 |  Test Acc: 94.45%\n",
      "Running model in eval mode with lang: cs\n",
      "data\\cs\n",
      "Number of testing examples: 10148\n",
      "Test Loss: 0.189 |  Test Acc: 94.45%\n"
     ]
    }
   ],
   "source": [
    "# czech\n",
    "run(\"train\", \"cs\", \"cs-model\")\n",
    "run(\"eval\", \"cs\", \"cs-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9f61c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: es\n",
      "data\\es\n",
      "Unique tokens in TEXT vocabulary: 17588\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 14187\n",
      "Number of validation examples: 1552\n",
      "Number of tokens in the training set: 382436\n",
      "Number of testing examples: 274\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t68694\t\t18.0%\n",
      "ADP\t\t62995\t\t16.5%\n",
      "DET\t\t53937\t\t14.1%\n",
      "PUNCT\t\t42218\t\t11.0%\n",
      "VERB\t\t36185\t\t 9.5%\n",
      "PROPN\t\t35112\t\t 9.2%\n",
      "ADJ\t\t22096\t\t 5.8%\n",
      "PRON\t\t12402\t\t 3.2%\n",
      "CONJ\t\t12262\t\t 3.2%\n",
      "ADV\t\t11031\t\t 2.9%\n",
      "NUM\t\t9812\t\t 2.6%\n",
      "SCONJ\t\t7095\t\t 1.9%\n",
      "AUX\t\t5335\t\t 1.4%\n",
      "X\t\t1778\t\t 0.5%\n",
      "SYM\t\t1452\t\t 0.4%\n",
      "PART\t\t32\t\t 0.0%\n",
      "The model has 2,394,210 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.386 | Train Acc: 55.96%\n",
      "\t Val. Loss: 0.509 |  Val. Acc: 84.22%\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.393 | Train Acc: 87.89%\n",
      "\t Val. Loss: 0.263 |  Val. Acc: 91.81%\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.248 | Train Acc: 92.32%\n",
      "\t Val. Loss: 0.219 |  Val. Acc: 93.07%\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.201 | Train Acc: 93.71%\n",
      "\t Val. Loss: 0.208 |  Val. Acc: 93.32%\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.176 | Train Acc: 94.44%\n",
      "\t Val. Loss: 0.196 |  Val. Acc: 93.80%\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.158 | Train Acc: 94.90%\n",
      "\t Val. Loss: 0.189 |  Val. Acc: 93.89%\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.145 | Train Acc: 95.32%\n",
      "\t Val. Loss: 0.188 |  Val. Acc: 94.02%\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.135 | Train Acc: 95.60%\n",
      "\t Val. Loss: 0.188 |  Val. Acc: 94.01%\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.125 | Train Acc: 95.92%\n",
      "\t Val. Loss: 0.189 |  Val. Acc: 93.97%\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.116 | Train Acc: 96.20%\n",
      "\t Val. Loss: 0.192 |  Val. Acc: 94.03%\n",
      "Test Loss: 0.204 |  Test Acc: 94.08%\n",
      "Running model in eval mode with lang: es\n",
      "data\\es\n",
      "Number of testing examples: 274\n",
      "Test Loss: 0.204 |  Test Acc: 94.08%\n"
     ]
    }
   ],
   "source": [
    "# spanish\n",
    "run(\"train\", \"es\", \"es-model\")\n",
    "run(\"eval\", \"es\", \"es-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28ce802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: ar\n",
      "data\\ar\n",
      "Unique tokens in TEXT vocabulary: 15889\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 6174\n",
      "Number of validation examples: 786\n",
      "Number of tokens in the training set: 225853\n",
      "Number of testing examples: 704\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t74156\t\t32.8%\n",
      "ADP\t\t33548\t\t14.9%\n",
      "ADJ\t\t23424\t\t10.4%\n",
      "CONJ\t\t19182\t\t 8.5%\n",
      "PUNCT\t\t17777\t\t 7.9%\n",
      "X\t\t17626\t\t 7.8%\n",
      "VERB\t\t17175\t\t 7.6%\n",
      "PRON\t\t10904\t\t 4.8%\n",
      "NUM\t\t6191\t\t 2.7%\n",
      "PART\t\t2996\t\t 1.3%\n",
      "DET\t\t1537\t\t 0.7%\n",
      "ADV\t\t827\t\t 0.4%\n",
      "SYM\t\t316\t\t 0.1%\n",
      "PROPN\t\t156\t\t 0.1%\n",
      "AUX\t\t31\t\t 0.0%\n",
      "INTJ\t\t7\t\t 0.0%\n",
      "The model has 2,224,310 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 1.965 | Train Acc: 37.43%\n",
      "\t Val. Loss: 1.323 |  Val. Acc: 58.85%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.791 | Train Acc: 74.76%\n",
      "\t Val. Loss: 0.428 |  Val. Acc: 86.89%\n",
      "Epoch: 03 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.316 | Train Acc: 90.04%\n",
      "\t Val. Loss: 0.253 |  Val. Acc: 92.13%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.186 | Train Acc: 94.17%\n",
      "\t Val. Loss: 0.205 |  Val. Acc: 93.41%\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.137 | Train Acc: 95.69%\n",
      "\t Val. Loss: 0.188 |  Val. Acc: 93.95%\n",
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.113 | Train Acc: 96.43%\n",
      "\t Val. Loss: 0.182 |  Val. Acc: 94.02%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.097 | Train Acc: 96.88%\n",
      "\t Val. Loss: 0.183 |  Val. Acc: 93.94%\n",
      "Epoch: 08 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.20%\n",
      "\t Val. Loss: 0.178 |  Val. Acc: 94.29%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.077 | Train Acc: 97.50%\n",
      "\t Val. Loss: 0.179 |  Val. Acc: 94.15%\n",
      "Epoch: 10 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.72%\n",
      "\t Val. Loss: 0.174 |  Val. Acc: 94.31%\n",
      "Test Loss: 0.162 |  Test Acc: 94.46%\n",
      "Running model in eval mode with lang: ar\n",
      "data\\ar\n",
      "Number of testing examples: 704\n",
      "Test Loss: 0.162 |  Test Acc: 94.46%\n"
     ]
    }
   ],
   "source": [
    "# arabic\n",
    "run(\"train\", \"ar\", \"ar-model\")\n",
    "run(\"eval\", \"ar\", \"ar-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f37b5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: af\n",
      "data\\af\n",
      "Unique tokens in TEXT vocabulary: 2235\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 1315\n",
      "Number of validation examples: 194\n",
      "Number of tokens in the training set: 33894\n",
      "Number of testing examples: 425\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t7335\t\t21.6%\n",
      "ADP\t\t4365\t\t12.9%\n",
      "DET\t\t3769\t\t11.1%\n",
      "PUNCT\t\t3129\t\t 9.2%\n",
      "VERB\t\t2957\t\t 8.7%\n",
      "PRON\t\t2495\t\t 7.4%\n",
      "AUX\t\t2276\t\t 6.7%\n",
      "ADJ\t\t2168\t\t 6.4%\n",
      "CCONJ\t\t1327\t\t 3.9%\n",
      "ADV\t\t1295\t\t 3.8%\n",
      "PART\t\t926\t\t 2.7%\n",
      "SCONJ\t\t716\t\t 2.1%\n",
      "PROPN\t\t359\t\t 1.1%\n",
      "SYM\t\t323\t\t 1.0%\n",
      "X\t\t291\t\t 0.9%\n",
      "NUM\t\t163\t\t 0.5%\n",
      "The model has 858,910 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.563 | Train Acc: 19.38%\n",
      "\t Val. Loss: 2.416 |  Val. Acc: 21.78%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.327 | Train Acc: 23.98%\n",
      "\t Val. Loss: 2.257 |  Val. Acc: 27.82%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.111 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.962 |  Val. Acc: 39.74%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.709 | Train Acc: 47.62%\n",
      "\t Val. Loss: 1.437 |  Val. Acc: 60.62%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.219 | Train Acc: 65.32%\n",
      "\t Val. Loss: 1.005 |  Val. Acc: 72.07%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.863 | Train Acc: 75.24%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 79.31%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.637 | Train Acc: 81.91%\n",
      "\t Val. Loss: 0.579 |  Val. Acc: 83.84%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.489 | Train Acc: 86.84%\n",
      "\t Val. Loss: 0.483 |  Val. Acc: 86.23%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train Acc: 89.67%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 88.38%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train Acc: 91.35%\n",
      "\t Val. Loss: 0.369 |  Val. Acc: 89.24%\n",
      "Test Loss: 0.368 |  Test Acc: 89.23%\n",
      "Running model in eval mode with lang: af\n",
      "data\\af\n",
      "Number of testing examples: 425\n",
      "Test Loss: 0.368 |  Test Acc: 89.23%\n"
     ]
    }
   ],
   "source": [
    "# afrikaans\n",
    "run(\"train\", \"af\", \"af-model\")\n",
    "run(\"eval\", \"af\", \"af-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "228e0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: lt\n",
      "data\\lt\n",
      "Unique tokens in TEXT vocabulary: 4392\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 2341\n",
      "Number of validation examples: 617\n",
      "Number of tokens in the training set: 47605\n",
      "Number of testing examples: 684\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t14933\t\t31.4%\n",
      "PUNCT\t\t8756\t\t18.4%\n",
      "VERB\t\t6604\t\t13.9%\n",
      "ADJ\t\t3274\t\t 6.9%\n",
      "CCONJ\t\t2136\t\t 4.5%\n",
      "ADV\t\t1826\t\t 3.8%\n",
      "PRON\t\t1688\t\t 3.5%\n",
      "ADP\t\t1490\t\t 3.1%\n",
      "NUM\t\t1312\t\t 2.8%\n",
      "DET\t\t1181\t\t 2.5%\n",
      "X\t\t1066\t\t 2.2%\n",
      "PROPN\t\t983\t\t 2.1%\n",
      "PART\t\t951\t\t 2.0%\n",
      "SCONJ\t\t917\t\t 1.9%\n",
      "AUX\t\t453\t\t 1.0%\n",
      "SYM\t\t26\t\t 0.1%\n",
      "INTJ\t\t9\t\t 0.0%\n",
      "The model has 1,074,867 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.364 | Train Acc: 27.48%\n",
      "\t Val. Loss: 2.163 |  Val. Acc: 31.87%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.088 | Train Acc: 38.19%\n",
      "\t Val. Loss: 1.942 |  Val. Acc: 43.91%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.713 | Train Acc: 48.38%\n",
      "\t Val. Loss: 1.501 |  Val. Acc: 53.13%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.290 | Train Acc: 58.78%\n",
      "\t Val. Loss: 1.215 |  Val. Acc: 61.05%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.956 | Train Acc: 68.84%\n",
      "\t Val. Loss: 0.981 |  Val. Acc: 68.26%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.721 | Train Acc: 76.91%\n",
      "\t Val. Loss: 0.836 |  Val. Acc: 73.18%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.575 | Train Acc: 81.82%\n",
      "\t Val. Loss: 0.756 |  Val. Acc: 76.78%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.478 | Train Acc: 84.81%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 78.48%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.410 | Train Acc: 87.07%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 78.99%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.365 | Train Acc: 88.51%\n",
      "\t Val. Loss: 0.636 |  Val. Acc: 79.61%\n",
      "Test Loss: 0.776 |  Test Acc: 76.53%\n",
      "Running model in eval mode with lang: lt\n",
      "data\\lt\n",
      "Number of testing examples: 684\n",
      "Test Loss: 0.776 |  Test Acc: 76.53%\n"
     ]
    }
   ],
   "source": [
    "# lithuanian\n",
    "run(\"train\", \"lt\", \"lt-model\")\n",
    "run(\"eval\", \"lt\", \"lt-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19605f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: hy\n",
      "data\\hy\n",
      "Unique tokens in TEXT vocabulary: 3800\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 1975\n",
      "Number of validation examples: 249\n",
      "Number of tokens in the training set: 42105\n",
      "Number of testing examples: 278\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t10524\t\t25.0%\n",
      "PUNCT\t\t8124\t\t19.3%\n",
      "VERB\t\t5355\t\t12.7%\n",
      "ADJ\t\t3317\t\t 7.9%\n",
      "AUX\t\t2963\t\t 7.0%\n",
      "CCONJ\t\t1905\t\t 4.5%\n",
      "ADV\t\t1843\t\t 4.4%\n",
      "PRON\t\t1636\t\t 3.9%\n",
      "DET\t\t1609\t\t 3.8%\n",
      "PROPN\t\t1499\t\t 3.6%\n",
      "ADP\t\t1301\t\t 3.1%\n",
      "SCONJ\t\t745\t\t 1.8%\n",
      "NUM\t\t540\t\t 1.3%\n",
      "PART\t\t458\t\t 1.1%\n",
      "X\t\t153\t\t 0.4%\n",
      "INTJ\t\t105\t\t 0.2%\n",
      "SYM\t\t28\t\t 0.1%\n",
      "The model has 1,015,667 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.448 | Train Acc: 23.17%\n",
      "\t Val. Loss: 2.221 |  Val. Acc: 36.59%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.204 | Train Acc: 32.60%\n",
      "\t Val. Loss: 2.026 |  Val. Acc: 39.28%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.908 | Train Acc: 41.49%\n",
      "\t Val. Loss: 1.567 |  Val. Acc: 50.81%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.448 | Train Acc: 54.44%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 65.37%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.095 | Train Acc: 65.32%\n",
      "\t Val. Loss: 0.899 |  Val. Acc: 72.42%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.861 | Train Acc: 72.89%\n",
      "\t Val. Loss: 0.731 |  Val. Acc: 77.40%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.691 | Train Acc: 78.46%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 81.02%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.561 | Train Acc: 82.78%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 83.08%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.472 | Train Acc: 85.42%\n",
      "\t Val. Loss: 0.499 |  Val. Acc: 84.06%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.418 | Train Acc: 87.06%\n",
      "\t Val. Loss: 0.475 |  Val. Acc: 85.24%\n",
      "Test Loss: 0.566 |  Test Acc: 81.32%\n",
      "Running model in eval mode with lang: hy\n",
      "data\\hy\n",
      "Number of testing examples: 278\n",
      "Test Loss: 0.566 |  Test Acc: 81.32%\n"
     ]
    }
   ],
   "source": [
    "# armenian\n",
    "run(\"train\", \"hy\", \"hy-model\")\n",
    "run(\"eval\", \"hy\", \"hy-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "946d76e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: ta\n",
      "data\\ta\n",
      "Unique tokens in TEXT vocabulary: 926\n",
      "Unique tokens in UD_TAG vocabulary: 15\n",
      "\n",
      "Number of training examples: 400\n",
      "Number of validation examples: 80\n",
      "Number of tokens in the training set: 6329\n",
      "Number of testing examples: 120\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t1860\t\t29.4%\n",
      "PROPN\t\t936\t\t14.8%\n",
      "VERB\t\t747\t\t11.8%\n",
      "PUNCT\t\t665\t\t10.5%\n",
      "ADJ\t\t466\t\t 7.4%\n",
      "AUX\t\t423\t\t 6.7%\n",
      "PART\t\t383\t\t 6.1%\n",
      "ADV\t\t251\t\t 4.0%\n",
      "ADP\t\t184\t\t 2.9%\n",
      "NUM\t\t156\t\t 2.5%\n",
      "PRON\t\t147\t\t 2.3%\n",
      "DET\t\t80\t\t 1.3%\n",
      "CCONJ\t\t31\t\t 0.5%\n",
      "The model has 727,239 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.468 | Train Acc: 22.40%\n",
      "\t Val. Loss: 2.251 |  Val. Acc: 29.06%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.219 | Train Acc: 29.48%\n",
      "\t Val. Loss: 2.151 |  Val. Acc: 29.22%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.180 | Train Acc: 28.35%\n",
      "\t Val. Loss: 2.099 |  Val. Acc: 29.06%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.107 | Train Acc: 30.08%\n",
      "\t Val. Loss: 2.062 |  Val. Acc: 29.06%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.073 | Train Acc: 31.08%\n",
      "\t Val. Loss: 1.999 |  Val. Acc: 34.44%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.017 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.990 |  Val. Acc: 35.63%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 2.004 | Train Acc: 36.12%\n",
      "\t Val. Loss: 1.926 |  Val. Acc: 37.85%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.931 | Train Acc: 38.19%\n",
      "\t Val. Loss: 1.874 |  Val. Acc: 38.64%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.844 | Train Acc: 39.47%\n",
      "\t Val. Loss: 1.809 |  Val. Acc: 41.41%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 1.799 | Train Acc: 41.13%\n",
      "\t Val. Loss: 1.745 |  Val. Acc: 43.55%\n",
      "Test Loss: 1.834 |  Test Acc: 42.25%\n",
      "Running model in eval mode with lang: ta\n",
      "data\\ta\n",
      "Number of testing examples: 120\n",
      "Test Loss: 1.834 |  Test Acc: 42.25%\n"
     ]
    }
   ],
   "source": [
    "# tamil\n",
    "run(\"train\", \"ta\", \"ta-model\")\n",
    "run(\"eval\", \"ta\", \"ta-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc666bb8",
   "metadata": {},
   "source": [
    "### Task 2 - Discussion (10 points)\n",
    "In this task we will discuss the received results from your evaluation. \n",
    "Each question has an additional markdown cell below for the answer. Please use it and put in you answer there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d293d04",
   "metadata": {},
   "source": [
    "##### Question 1: How the performance changes accross language families and available dataset size? Make a conclusion of how the model's prediction depends on the available data. (2 point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00edb43",
   "metadata": {},
   "source": [
    "According to the received results we can see that the high-resource languages show better performance than the low-resource. Mainly, the dataset size and available training data have an impact on the computational results. The more data is available for the language the better the model predicts the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bad4ba",
   "metadata": {},
   "source": [
    "##### Question 2: What role does the training set size plays for the model? Which problem regarding training sets occurs when you deal with the low-resource languages? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61f5f3",
   "metadata": {},
   "source": [
    "Training set size determines how good the model will be trained. Unfortunately, the amount of available data for the low-resource languages is lower than for the high-resource. Therefore, it is difficult to train the model so that it can provide a good prediction for the given input sequense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec7e44",
   "metadata": {},
   "source": [
    "##### Question 3: What do the parameters \"n_layers\", \"bidirectional\" and \"dropout\" (variable \"params\" in the first code cell) of the LSTM model mean? According to your research results please answer the following questions regarding the low-resource languages: (4 points)\n",
    "#### - What happens when you increase the variable n_layers and why? \n",
    "#### - What changes when the model is unidirectional and why? \n",
    "#### - What happens when you increase the dropout and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a27968",
   "metadata": {},
   "source": [
    "n_layers:\n",
    "This parameter determines how many LSTM networks are used and connected together so that one LSTM takes the output of another one as an input. Combining more LSTM networks may lead to the greather results but they need a lot of high-quality training data as well. Usually 2 networks are enough to deal with the complex labels.\n",
    "Since it isn't enough available training data for the low-resource languages, increasing the number of the LSTM networks should lead to poorer results. \n",
    "\n",
    "bidirectional:\n",
    "Using bidirectional or unidirectional model.\n",
    "Bidirectional LSTM model, as the name says, has two directions, in which they can extract information from the input sequence: forward and backward. By this way it can store the results in his hidden states to look up into it at the any time for generating the output. Combining the forward and backward directions usually produceses better results. Unidirectional model has only one forward direction, thus the predicition of labels contains more errors.\n",
    "Applying an unidirectional LSTM model to train on the data for the low-resource language leads to poorer results.\n",
    "\n",
    "dropout:\n",
    "Dropout determines the probability of dropout for each single node in the LSTM network. That means the output from the node will be completely ignored for the next round. This concept is important to prevent overfitting so that the features learned in the first rounds don't have a significant impact on the next rounds, i.e. later learned features can be easilly stored in the model's memory.\n",
    "Increasing the dropout for the low-resource languages means ignoring more features when training. This leads to poorer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f2eba",
   "metadata": {},
   "source": [
    "##### Question 4: Define the term \"label noise\". After that, please answer what happens if the labels in the training data are noisy? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc54ce",
   "metadata": {},
   "source": [
    "Sometimes we don't have any access to the high-quality labeled training data. In this case, to train the model, we need to deal with so called noisy labeled data, i.e. data that contains errors. Applying such data as the training dataset for the model will cause significant errors by predicting the labels."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c292fd389ce31484c1965e4c1a08fce32726d8741c1350fb6a4117012f73eb75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
