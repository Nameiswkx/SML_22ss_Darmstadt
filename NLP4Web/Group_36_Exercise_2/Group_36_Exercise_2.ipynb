{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h1> NLP and the Web: Home Exercise 2 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in class, <b>spaCy</b> is a useful open-source library that enables the user to perform several NLP tasks with high-quality results. It is not only helpful for beginners in NLP but also for advanced programmers who want to integrate NLP features into real products.\n",
    "\n",
    "For this exercise, you should only use spaCy; but you may use numpy and pandas if needed. Of course, you are also allowed to use the entire [Python Standard Library](https://docs.python.org/3.9/library/index.html). Please follow the instructions given below. In case of questions, use our Discussion forum in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tut mir leid\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from typing import List, Mapping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# download the language model if you haven't already (you may have to restart your Python kernel)\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# this is a bit of a hacky patch for spaCy's missing handling of contractions (haven't, she'll, I'm) in version 3\n",
    "# don't worry about it\n",
    "try:\n",
    "    nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"n't\"}]], {\"LEMMA\": \"not\"})\n",
    "    nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'ll\"}]], {\"LEMMA\": \"will\"})\n",
    "    nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'ve\"}]], {\"LEMMA\": \"have\"})\n",
    "    nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'m\"}]], {\"LEMMA\": \"be\"})\n",
    "except KeyError:\n",
    "    print('Tut mir leid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information\n",
    "We tried to make the description of the parameters as clear as possible. However, if you believe that something is missing, please reach out to us in Moodle and we will try to help you out.  \n",
    "\n",
    "We provide type hints for function parameters and return values of functions that you have to implement in the tasks. These are suggestions only, and you may use different types if you prefer. As long as you produce the required output in a coherent and understandable way, you can get full points. \n",
    "\n",
    "We use the term 'array-like object' to loosely refer to collection types like lists, arrays, maps, dataframes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Corpus\n",
    "\n",
    "In this exercise, you will work with a real dataset of public english-language tweets for the keyword 'lockdown' posted between Dec 14th and Dec 22nd 2020. It was originally collected for use in a psychological experiment investigating the public perception of covid lockdowns.  \n",
    "\n",
    "Tweets were scraped from Twitter search results using the [snscrape](https://github.com/JustAnotherArchivist/snscrape) tool on Dec 22nd 2020. All links and @mentions were removed. The subset of the corpus you are working on has been further trimmed down to reduce spam and off-topic content present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 5 Points\n",
    "To get started, you will have to read the dataset from the provided `tweets.txt` file. Each line in this file represents a single tweet. You will need to open and read the file before starting the other subtasks.\n",
    "\n",
    "**Hint 1**: Depending on how you read the dataset, you may have to remove linebreaks from the end of the tweets. You can use the [`rstrip`](https://docs.python.org/3.9/library/stdtypes.html) function to do so.  \n",
    "**Hint 2**: You may have to select 'utf-8' as the encoding when opening the file.  \n",
    "**Hint 3**: For this task you have to use some spaCy functions. You can find some useful information about spaCy tokens and their attributes [here](https://spacy.io/api/token). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Tokenize each tweet in the dataset, then print the tokenized versions of the first five tweets (\"token1\", \"token2\", \"token3\"...). Use spaCy to solve this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token0:  [Nothing, stopping, the, nphet, disciples, from, following, their, dogma, in, private, ., No, one, is, forcing, you, and, your, fellow, cult, members, to, not, lockdown, and, follow, your, Leaders]\n",
      "----------------------------------------\n",
      "token1:  [After, making, it, through, our, tough, lockdown, in, Melbourne, ,, I, 'll, be, missing, my, son, at, Xmas, ,, police, officer, deployed, up, to, the, border, checkpoints, until, Sunday, ., Wish, you, guys, in, Sydney, wore, masks, ðŸ˜·]\n",
      "----------------------------------------\n",
      "token2:  [Seriously, ,, another, variant, of, the, SARS, Cov-2, perhaps, a, mutated, SARS, Cov-3, virus, would, be, inimical, to, world, growth, only, heaven, knows, if, the, vaccine, would, be, effective, for, the, new, variant, whichsoever, is, a, doubt, ,, I, only, pray, another, phase, of, global, lockdown, does, n't, looms]\n",
      "----------------------------------------\n",
      "token3:  [How, about, reply, to, creators, that, need, your, help, to, access, their, accounts, with, you]\n",
      "----------------------------------------\n",
      "token4:  [No, ,, our, country, is, in, lockdown, for, -+1000, deaths, and, lockdown, does, nâ€™t, fit, in, the, conditions, of, christmas, ., We, ca, nâ€™t, even, stay, with, each, other, at, my, home,  , to, celebrate, christmas, because, then, we, spread, the, corona, shit, ., Hope, you, have, a, merry, christmas, Mongraal, ..]\n",
      "----------------------------------------\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read file\n",
    "with open('tweets.txt','r',encoding='utf-8') as f:\n",
    "    contents=list(sentence.rstrip() for sentence in f.readlines())\n",
    "\n",
    "    \n",
    "docs=list(nlp(tweet) for tweet in contents)\n",
    "for i in range(5):\n",
    "    print('tweet'+str(i)+': ',list(docs[i]))\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Implement the function `occurence_lowercase`. It shall calculate the (absolute) number of occurrences of each token that is in lowercase. Apply the function to our dataset of tweets and print the result (i.e. 'token: occurence') in descending order. Use spaCy to identify lowercased tokens.\n",
    "\n",
    "**Hint**: Do not lowercase all tokens, instead identify and count all already lowercased tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 1111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(lockdown, 998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(to, 799)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(and, 609)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a, 608)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>(attempts, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>(digibash, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>(hotrod, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>(buyer, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>(seller, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4226 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             elements\n",
       "0         (the, 1111)\n",
       "1     (lockdown, 998)\n",
       "2           (to, 799)\n",
       "3          (and, 609)\n",
       "4            (a, 608)\n",
       "...               ...\n",
       "4221    (attempts, 1)\n",
       "4222    (digibash, 1)\n",
       "4223      (hotrod, 1)\n",
       "4224       (buyer, 1)\n",
       "4225      (seller, 1)\n",
       "\n",
       "[4226 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def occurence_lowercase(data: List[List[str]]) -> Mapping[str, int]:\n",
    "    \"\"\"\n",
    "    Counts occurences of all lowercased tokens.\n",
    "    The type hints are suggestions only. Feel free to use whatever works for you.\n",
    "    \n",
    "    @param data: array-like object containing tokenized tweets from subtask a)\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    lower=[]\n",
    "    for doc in data:\n",
    "        for token in doc:\n",
    "            if token.is_lower:\n",
    "                if token.text in lower:\n",
    "                    dic[token.text]+=1\n",
    "                    \n",
    "                else:\n",
    "                    dic[token.text]=1\n",
    "                    lower.append(token.text)\n",
    "    dic=sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['elements']=occurence_lowercase(docs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Implement the function `occurence_no_punctuation`. It shall extract all tokens which occur five or more times, excluding punctuation. Additionally it shall return the absolute occurence of these tokens similar to b). Apply the function to our dataset of tweets and print the result in descending order. Use spaCy to identify which tokens are considered to be punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(the, 1111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(lockdown, 998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(to, 799)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(and, 609)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a, 608)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>(test, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>(yes, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>(showing, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>(local, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>(Where, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>834 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            elements\n",
       "0        (the, 1111)\n",
       "1    (lockdown, 998)\n",
       "2          (to, 799)\n",
       "3         (and, 609)\n",
       "4           (a, 608)\n",
       "..               ...\n",
       "829        (test, 5)\n",
       "830         (yes, 5)\n",
       "831     (showing, 5)\n",
       "832       (local, 5)\n",
       "833       (Where, 5)\n",
       "\n",
       "[834 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def occurence_no_punctuation(data: List[List[str]]) -> Mapping[str, int]:\n",
    "    \"\"\"\n",
    "    Counts occurences of all tokens excluding punctuation and returns all with occurences greater or equal 5.\n",
    "    The type hints are suggestions only. Feel free to use whatever works for you.\n",
    "    \n",
    "    @param data: array-like object containing tokenized tweets from subtask a)\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    dic={}\n",
    "    words=[]\n",
    "    for doc in data:\n",
    "        for token in doc:\n",
    "            if token.is_punct==False and token.is_space==False:\n",
    "                if token.text in words:\n",
    "                    dic[token.text]+=1\n",
    "                else:\n",
    "                    dic[token.text]=1\n",
    "                    words.append(token.text)\n",
    "    dic={k:v for k,v in dic.items() if v>=5}\n",
    "    dic=sorted(dic.items(), key=lambda item:item[1], reverse=True)\n",
    "    return dic\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['elements']=occurence_no_punctuation(docs)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Explain the internal structure of spaCy with respect to tokens in at most ~5 sentences. Make sure you explain the three main data components. Please refer to the notebook from the practice class or the  [spaCy documentation](https://spacy.io/api). You may want to use an example to show how a given token is stored / represented in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** This information is stored as a 64-bit hash value, and we can access it from any location and any object in spaCy, such as npl.vocab.strings, doc.vocab.strings, or span.doc.vocab.string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And 12172435438170721471\n",
      "This is my desired string: And\n",
      "This is my desired hash: 12172435438170721471\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"And it starts with understanding this: Even as the Delta variant 19 [sic] has â€” COVID-19 â€” has been hitting this country hard, we have the tools to combat the virus, if we can come together as a country and use those tools.\"\n",
    "doc = nlp(text)\n",
    "w =[]\n",
    "for token in doc[0:5]:\n",
    "    w.append(token)\n",
    "#the component for document\n",
    "new_doc = nlp(text)\n",
    "#the component for vocabulary\n",
    "voca = nlp.vocab\n",
    "#the look up table\n",
    "lexeme = voca[new_doc[0].text]\n",
    "print(lexeme.text, lexeme.orth,)\n",
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Use the spaCy matcher to find groups of tokens that are similar in some way. You can decide for yourself what kind of 'similarity' is interesting to you. You should first create your pattern(s) and then use them to find matches in the tweet dataset. For each match, print both the match itself as well as the sentence (not necessarily the entire tweet!) it was found in. Briefly describe how you chose your pattern and your motivation for doing so (why is it relevant / useful / interesting?) in ~2 sentences.\n",
    "\n",
    "The output for a match could look something like this:\n",
    "```\n",
    "This is the sentence of the match.\n",
    "    - the match\n",
    "```\n",
    "\n",
    "**Hint**: You can check out [explosion.ai/demos/matcher](https://explosion.ai/demos/matcher) to play around with different patterns. You can also refer to the [spaCy documentation of the Token class](https://spacy.io/api/token) for interesting attributes and the [spaCy matching documentation](https://spacy.io/usage/rule-based-matching/) for info on how to create patterns.\n",
    "\n",
    "**Example 1**: All tokens that describe a date or time. 'Let's meet this evening.' ==> *this evening*  \n",
    "**Example 2**: Tokens describing appearance (e.g. adjectives after 'look'): 'It looks good.' ==> *looks good*\n",
    "\n",
    "*Sidenote: Recall that the dataset you're working on was originally collected for a psychological study investigating differences in the public perception of the first vs. the later lockdowns. Perhaps you can explore aspects of that with your pattern?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_attr = [{'POS': 'ADJ'},{'POS':'NOUN'}]\n",
    "matcher.add('PATTERN_ATTR', [pattern_attr])\n",
    "\n",
    "#pun=['\\.','?',';','!']\n",
    "text=\" \"\n",
    "f=open(\"tweets.txt\",mode='r',encoding='utf-8')\n",
    "for line in f.readlines():\n",
    "    text+=line.strip('\\n')\n",
    "#text=\"Nothing stopping the nphet disciples from following their dogma in private.I just want businesses open and students in class Eoin I'm not in the population of \\\"non frontline public sector workers\\\"...does that make me  more likely to fall into the bracket of being pro extreme lockdown, given\"\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"the match:\",doc[start:end].text)\n",
    "    for sentence in doc.sents:\n",
    "        if doc[start:end].text in sentence.text:\n",
    "            print(sentence.text+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Briefly describe another use case / pattern which you could have implemented and why it could be useful or interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** These are the tokens, which are all in the form of adjectives + nouns. We think the best way to learn the changes in people's psychology, is to know how people describe the thing. Therefore, we think to learn adjectives and nouns at the same time, it is helpful to understand whether people have a positive or negative attitude towards before and after the lockdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Use the spaCy matcher to extract and print [proper nouns](https://en.wikipedia.org/wiki/Proper_and_common_nouns) that are longer than one token. Print each tweet from the dataset that contains at least one such proper noun together with all the matching proper nouns it contains.\n",
    "\n",
    "The output for a given tweet should look something like this:  \n",
    "```\n",
    "I live in New York City and I like Hot Dogs & Coke\n",
    "    - New York City\n",
    "    - Hot Dogs\n",
    "```\n",
    "\n",
    "**Hint 1**: If there is a proper noun like 'New York City' you should only print 'New York City' and not 'New York', 'New York City', and 'York City'. As in the previous task, you can quickly test different patterns using [explosion.ai/demos/matcher](https://explosion.ai/demos/matcher).\n",
    "\n",
    "**Hint 2**: For this task you may have to use some functions that are not provided by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.1 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROPN</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christmas Mongraal</td>\n",
       "      <td>(No, ,, our, country, is, in, lockdown, for, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Storm Oregon</td>\n",
       "      <td>(Get, their, names, and, make, sure, they, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State Capitol</td>\n",
       "      <td>(Get, their, names, and, make, sure, they, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>(It, was, a, draconian, lockdown, that, would,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>East Asia</td>\n",
       "      <td>(It, was, a, draconian, lockdown, that, would,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>B4 lockdown</td>\n",
       "      <td>(Na, the, Koko, ., B4, lockdown, go, reach, 6m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Us Ontarians</td>\n",
       "      <td>(LOL, !, Where, are, you, ?, Our, snow, has, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Ye Olde Lockdown</td>\n",
       "      <td>(i, am, ALSO, very, excited, during, Ye, Olde,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>david liebe hart</td>\n",
       "      <td>(Just, thinking, about, how, I, almost, saw, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Dr. Jeanne Noble</td>\n",
       "      <td>(Even, as,  , have, made, improvements, ,, inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PROPN                                              tweet\n",
       "0    christmas Mongraal  (No, ,, our, country, is, in, lockdown, for, -...\n",
       "1          Storm Oregon  (Get, their, names, and, make, sure, they, do,...\n",
       "2         State Capitol  (Get, their, names, and, make, sure, they, do,...\n",
       "3           South Korea  (It, was, a, draconian, lockdown, that, would,...\n",
       "4             East Asia  (It, was, a, draconian, lockdown, that, would,...\n",
       "..                  ...                                                ...\n",
       "217         B4 lockdown  (Na, the, Koko, ., B4, lockdown, go, reach, 6m...\n",
       "218        Us Ontarians  (LOL, !, Where, are, you, ?, Our, snow, has, n...\n",
       "219    Ye Olde Lockdown  (i, am, ALSO, very, excited, during, Ye, Olde,...\n",
       "220    david liebe hart  (Just, thinking, about, how, I, almost, saw, d...\n",
       "221    Dr. Jeanne Noble  (Even, as,  , have, made, improvements, ,, inc...\n",
       "\n",
       "[222 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pattern = [\n",
    "            {'POS': 'PROPN', 'OP': '!'},\n",
    "            {'POS': 'PROPN', 'OP': '+', 'DEP': 'compound'},\n",
    "            {'POS': 'PROPN'},\n",
    "            {'POS': 'PROPN', 'OP': '!'}\n",
    "            ]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"propnoun\",[pattern])\n",
    "matches=matcher(doc)\n",
    "\n",
    "pn=[]\n",
    "tweets=[]\n",
    "df=pd.DataFrame()\n",
    "for doc in docs:\n",
    "    matches=matcher(doc)\n",
    "    for match_id,start,end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start+1:end-1]  # The matched span\n",
    "        pn.append(span.text)\n",
    "        tweets.append(doc)\n",
    "        \n",
    "\n",
    "df['PROPN']=pn\n",
    "df['tweet']=tweets\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Go over the entire dataset and find verbs and nouns (including proper nouns!) that share the same lemma. Print each lemma that is shared between at least one verb and one noun together with all distinct, lowercased, non-lemmatized nouns and verbs from the dataset that share that lemma. Every lemma should only be printed once.\n",
    "\n",
    "The output for the lemma 'walk' may look like this...\n",
    "```\n",
    "lemma: walk; nouns: walk; verbs: walk, walking, walked;\n",
    "```\n",
    "... assuming the dataset contains a sentence like 'We walked (V) the walk (N) and still walk (V) it today. Walking (V) brings us great joy.'\n",
    "\n",
    "**Hint**: For this task you may need some functions which are not provided by spaCy. For example, you can join two dataframes and group strings by concatenating them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>share</td>\n",
       "      <td>{share}</td>\n",
       "      <td>{sharing, share}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lockdown</td>\n",
       "      <td>{Lockdown}</td>\n",
       "      <td>{Lockdown}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May</td>\n",
       "      <td>{May}</td>\n",
       "      <td>{May}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>line</td>\n",
       "      <td>{line, lines}</td>\n",
       "      <td>{lining}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>{post, posts}</td>\n",
       "      <td>{post, posted}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>room</td>\n",
       "      <td>{room}</td>\n",
       "      <td>{rooming}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>wanna</td>\n",
       "      <td>{wanna}</td>\n",
       "      <td>{wanna}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>test</td>\n",
       "      <td>{tests, test}</td>\n",
       "      <td>{tested}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>back</td>\n",
       "      <td>{backs, back}</td>\n",
       "      <td>{back, backed}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>sign</td>\n",
       "      <td>{sign}</td>\n",
       "      <td>{Sign, sign}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma          nouns             verbs\n",
       "0       share        {share}  {sharing, share}\n",
       "1    Lockdown     {Lockdown}        {Lockdown}\n",
       "2         May          {May}             {May}\n",
       "3        line  {line, lines}          {lining}\n",
       "4        post  {post, posts}    {post, posted}\n",
       "..        ...            ...               ...\n",
       "230      room         {room}         {rooming}\n",
       "231     wanna        {wanna}           {wanna}\n",
       "232      test  {tests, test}          {tested}\n",
       "233      back  {backs, back}    {back, backed}\n",
       "234      sign         {sign}      {Sign, sign}\n",
       "\n",
       "[235 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test='We walked the walk and still walk it today. Walking brings us great joy.'\n",
    "tokens=[]\n",
    "lemma=[]\n",
    "pos=[]\n",
    "nouns=[]\n",
    "df=pd.DataFrame()\n",
    "\n",
    "for doc in docs:\n",
    "    for t in doc:\n",
    "        tokens.append(t.text)\n",
    "        lemma.append(t.lemma_)\n",
    "        pos.append(t.pos_)\n",
    "df['tokens']=tokens\n",
    "df['lemma']=lemma\n",
    "df['pos']=pos\n",
    "\n",
    "is_verb_and_noun = lambda x: set(x) == set(['VERB', 'NOUN'])\n",
    "\n",
    "out = df.loc[df.groupby('lemma')['pos'].transform(is_verb_and_noun), 'lemma']\n",
    "\n",
    "groups_multiple=df.groupby(['lemma','pos'])\n",
    "group=df.groupby('lemma')\n",
    "\n",
    "lemma_v=set(gm[0][0] for gm in groups_multiple if gm[0][1]=='VERB')\n",
    "lemma_n=set(gm[0][0] for gm in groups_multiple if gm[0][1]=='NOUN')\n",
    "lemma_pn=set(gm[0][0] for gm in groups_multiple if gm[0][1]=='PROPN')\n",
    "lemma_vn=set(list(lemma_v & (lemma_n | lemma_pn)))\n",
    "\n",
    "verbList=[]\n",
    "nounsList=[]\n",
    "for lem in lemma_vn:\n",
    "    verbs=groups_multiple.get_group((lem,'VERB'))\n",
    "    try:\n",
    "        nouns=groups_multiple.get_group((lem,'NOUN'))\n",
    "    except KeyError:\n",
    "        nouns=groups_multiple.get_group((lem,'PROPN'))\n",
    "    \n",
    "    verbList.append(set(list(verbs['tokens'])))\n",
    "    nounsList.append(set(list(nouns['tokens'])))\n",
    "df=pd.DataFrame()\n",
    "df['lemma']=list(lemma_vn)\n",
    "df['nouns']=nounsList\n",
    "df['verbs']=verbList\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Use spaCy to find named entities representing organizations in the dataset. Print all (distinct) entities you find.\n",
    "\n",
    "\n",
    "**Hint**: You can find NER tags available in spaCy's models in the [model documentation](https://spacy.io/models/en#en_core_web_sm). While you *could* use the Matcher again for this task, it is much easier to access the already parsed named entities of a document. You may want to refer to the [document documentation](https://spacy.io/api/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(hbu)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(BOTH)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Royals)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Quizzes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(BCG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>(PCR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>(Covid)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>(Paddy)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>(Covid)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>(QLD)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity\n",
       "0        (hbu)\n",
       "1       (BOTH)\n",
       "2     (Royals)\n",
       "3    (Quizzes)\n",
       "4        (BCG)\n",
       "..         ...\n",
       "261      (PCR)\n",
       "262    (Covid)\n",
       "263    (Paddy)\n",
       "264    (Covid)\n",
       "265      (QLD)\n",
       "\n",
       "[266 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities=list(entity for doc in docs for entity in doc.ents if entity.label_=='ORG')\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['Entity']=entities\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) The dataset you've been working with contains real, messy web data. Looking at the output from subtask a), name three difficulties related to working with this kind of social media content rather than, for example, data from Wikipedia and give examples for each.\n",
    "\n",
    "\n",
    "**Hint**: See where spaCy fails. It may be helpful to inspect the context of the named entities you found above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "* abbreviation. e.g. Nov/Dec, which Spacy recognize it as ORG. It can also represent as time november and december.\n",
    "* Emoji. Spacy recoginize it as ORG, but it is not clear if it is a name of ORG or it is just a emoji.\n",
    "* word with all upppercase alphabets. e.g.\"BORING\" it can be ORG also can represent emotion boring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload in Moodle your working Jupyter-Notebook **before next exercise session** <span style=\"color:red\">(Nov 25, 16:14pm)</span>. Submission format: Group_No_Exercise_No.zip<br>\n",
    "Submission should contain your filled out Jupyter notebook (naming schema: Group_No_Exercise_No.ipynb) and any auxiliar files that are necessary to run your code (e.g. datasets provided by us).  \n",
    "Each submission must only be handed in once per group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
