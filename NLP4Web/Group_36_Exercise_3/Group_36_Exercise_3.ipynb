{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> NLP and the Web - WS 21/22: Home Exercise 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>General Information</h3>\n",
    "<br>\n",
    "We tried to make the description of the parameters as clear as possible. However, if you believe that something is missing, please reach out to us in Moodle and we will try to help you out. <br><br>\n",
    "We provide type hints for function parameters and return values of functions that you have to implement in the tasks. These are suggestions only, and you may use different types if you prefer. As long as you produce the required output in a coherent and understandable way, you can get full points. <br><br>\n",
    "We use the term 'array-like object' to loosely refer to collection types like lists, arrays, maps, dataframes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 1: Hidden Markov Models - 10 Points </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will be able to perform part-of-speech tagging by yourself using Hidden Markov Models.\n",
    "Part-of-speech tagging (POS tagging) is the process assigning a syntactic label to each token in a document. This\n",
    "kind of tagging gives us more information about surrounding words, e.g. adjectives occur more often before a noun and\n",
    "after a determiner and verbs appear usually after a noun. As an example, here we have two sentences:<br><br>\n",
    "\"Time flies like an arrow.\"<br>\n",
    "\"Fruit flies ate a banana\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer would split them into the following tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A part-of-speech tagger (spaCy tagger) could then assign labels, or <b>tags</b>, to the tokens according to their respective parts\n",
    "of speech:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models (HMMs): are graphical probabilistic models that enable us to talk about observed and hidden\n",
    "events. The latter being considered as causal factors in the model (Jurafky & Martin, 2019). For instance, in our real\n",
    "world we can see words but we cannot see pos-tags. Given the explanation of HMMs in the lecture and in our lab session,\n",
    "we will find the best pos sequence for a given word sequence using the Bayes’ Rule and the Viterbi Algorithm, please\n",
    "refer to slides from both Lecture and Lab session for further details.<br><br>\n",
    "<b>Note:</b> For this exercise, you may only use spaCy, scikit-learn, NumPy, Pandas and internal packages from Python. Please\n",
    "follow the instructions as given below and in case of questions use our Discussion forum in Moodle, we don’t answer\n",
    "questions via email.\n",
    "<br>\n",
    "<br>\n",
    "Using the files <i>transisition_probabilities.csv</i> and <i>observation_likelihoods.csv</i>, compute the likelihood of POS tags for the\n",
    "following word sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJMAAAAoCAYAAAD66MijAAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AAAT7SURBVHic7dtNSFRvFAbw544m5AjW4AfiRvxoYwUqWOpikBwUroRoxKWdpiGioEuRtHGh1CJdVIq4iiFwERGDEoFrHQShDwpFi2iYRPE6oiHFXJ//ohrKv6NOvlYj5wezGK/vuWcOj77X64xGkhBCAdvfbkAcHxImoYyESSgjYRLKSJiEMhImoYyESajDKDmdTgL47UdPT4/ymkfxcDqdR/L6Y20ukeawG42M7qalpmmIcsm+aw9T86j8yV7/5blE04Nsc0IZCZNQRsIklJEwCWUkTEIZCZNQRsIklFEapvHxcbhcLhiGgdevX6ssLWKA0jDpuo6UlBSUl5fj7NmzKksrNzIyEhM1Y0n8325gLyThdrthmiauXbuGixcvYmBgAIZh4PTp0+jr60N8fDy6u7sxPj6Oubk5zM7OoqWlBTabDR6PB7qu4+nTpwiFQrhz5w4cDgcmJibQ2dmJjY0NZGZmwjCMQ/e6W823b9/i4cOH0DQNZWVl0HVdwVS+mZ6ehsfjQVtbG27fvo2mpiaYpvnLDEpLSwEA8/Pz8Hg8OHHiBEpLS3Hp0iXMzc1hdHQUa2traGpqwoULFw7f1IH/8fLdfksMw+Dw8HBUa/equbi4yKysLJLk+vo609LSeP/+fZLkrVu3uLCwwIWFBWZmZpIkJycnWVxczJWVFZ45c4ZDQ0N8//49a2tree/ePZLk2toaExMTubS0xNXVVSW97qzp9/uZm5vL5eVlhkIhVlVV8fHjx0rORTL8+trb2/n8+XN6vd7/zYAk/X4/8/Pzubq6Sp/Px6KiIgYCARYUFNA0TQYCAaalpfHz589R97DTP/2bCQCys7Nht9sxPz+Pqakp9Pf3Y2xsDC0tLfjw4QNycnIAAH6/H0tLS3j16hVM00RKSgoSEhLgcrmQlZUFXdcxMzMDADh16hRsNhvS09OV9bmz5t27d+F0OpGamgoAqK+vx8jICGpra5Wc78fra21tjTgDABgbG0NFRQUcDgeKi4sxMzODwcFBnDx5Eg8ePAAA2O12vHv37tCXJjHx19zly5fx7NkzzM7OoqGhAR8/fsSLFy+Ql5cHAPj06ROuXLmCiYkJuFyuXWvExcX9yZaxvr4Ou90efp6UlIRgMHhk54s0g2AwiISEhPBzTdNgmiYqKyvR1dWFrq4uJUECYihMHo8HGRkZAIDq6mo0Nzejrq4OwLefvtTUVDQ0NCApKelANS3LwubmptI+f67pdDrh8/nCx3w+H8rLy5We72eRZlBWVgav14utra1wjyUlJeHryB9f+3H8UA68IR5gD/V6vczLy2NFRQVfvnx54LX7tWFZFtPT07m4uEiSnJqa4vnz58PH37x5w3PnzrG5uZm9vb1MTk7m9evX6XA4ePPmTQaDQdbU1LCwsDDcl67rrKys5KNHj6Lqaa9ed9Z0u928ceMG3W43DcPgxsaGsnN5vV46HA52dHTQNM1dZ/DkyROSZGdnJ/Pz83n16lXW19eTJDs6OlhUVMTGxkY2NjYyEAhE3cNOx+b9TNvb2yAZ1XZGEpqm7Xrsd3vdWTMUCuHr169ITEyMuEbVXPaagWVZsCzrly3vy5cvsCzrt3rb9XuPS5hUkzfHRd9DTFwzidggYRLKSJiEMhImoYyESSgjYRLKSJiEOge+vfmdfKJXPtEbSdQ3LYWIRLY5oYyESSgjYRLKSJiEMv8B85NY3z0YmFEAAAAASUVORK5CYII=\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Please use comments where appropriate to help tutors understand your code.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1.1 - 4 points</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the given files and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tags      VB       TO       NN     PPSS\n",
      "0   <s>  0.0190  0.00430  0.04100  0.06700\n",
      "1    VB  0.0038  0.03500  0.04700  0.00700\n",
      "2    TO  0.8300  0.00000  0.00047  0.00000\n",
      "3    NN  0.0040  0.01600  0.08700  0.00450\n",
      "4  PPSS  0.2300  0.00079  0.00120  0.00014\n",
      "   Tags     I      want   to      race\n",
      "0    VB  0.00  0.009300  0.00  0.00012\n",
      "1    TO  0.00  0.000000  0.99  0.00000\n",
      "2    NN  0.00  0.000054  0.00  0.00057\n",
      "3  PPSS  0.37  0.000000  0.00  0.00000\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "observation_likelihood = pd.read_csv('observation_likelihoods.csv')\n",
    "transition_probability = pd.read_csv('transisition_probabilities.csv')\n",
    "\n",
    "# Assign labels for better readability\n",
    "observation_likelihood.columns = ['Tags', ' I', ' want', ' to ', ' race']\n",
    "transition_probability.columns = ['Tags', 'VB', 'TO', 'NN', 'PPSS']\n",
    "\n",
    "print(transition_probability)\n",
    "print(observation_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a function to obtain a list of all possible tag sequences for the word sequence given above and store them in a list. Print the first 5 tag sequences as well as the total number of sequences. <br>\n",
    "<b>Note</b>: We do not consider the observation and transition probabilities yet. Tags can appear multiple times in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VB', 'VB', 'VB', 'VB'), ('VB', 'VB', 'VB', 'TO'), ('VB', 'VB', 'VB', 'NN'), ('VB', 'VB', 'VB', 'PPSS'), ('VB', 'VB', 'TO', 'VB')]\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def calculate_tag_permutations(observations: pd.DataFrame) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Calculates an array-like object of all possible tag permutations for the token sequence\n",
    "    \n",
    "    :param observations: array-like object of tokens and tag probabilities\n",
    "    :return: array-like object of all tag permutations\n",
    "    \"\"\"\n",
    "    return list(itertools.product(observations['Tags'], repeat=len(observations)))\n",
    "all_tag_seq=calculate_tag_permutations(observation_likelihood)\n",
    "print(all_tag_seq[:5])\n",
    "print(len(all_tag_seq))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement the Hidden Markov Model (HMM) discussed in the lecture and use it to assign a probability to each possible sequence. Print the most likely tag sequence and its probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "def calculate_sequence_probability(sequence: List[str], \n",
    "                                   observations: pd.DataFrame, transitions: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a probability for a token sequence using the Hidden Markov Model\n",
    "    \n",
    "    :param sequences: array-like object of POS tag sequences\n",
    "    :param observations: array-like object of observation probabilities\n",
    "    :param transitions: array-like object of transition probabilities\n",
    "    :return: Probability of the POS sequence\n",
    "    \"\"\"\n",
    "    #print(sequence)\n",
    "    sent=observation_likelihood.columns[1:]\n",
    "    #print(sent)\n",
    "    for num,token in enumerate(sequence):\n",
    "        #print(\"num,token:\",num,token)\n",
    "        if num==0:\n",
    "            pro=float(transitions[token][num]*observations[sent[num]][observations['Tags']==token])\n",
    "        else:\n",
    "            last=sequence[num-1]\n",
    "            pro=pro*float(transitions[token][transitions['Tags']==last])*float(observations[sent[num]][observations['Tags']==token])\n",
    "    return pro\n",
    "                                                                                      \n",
    "            \n",
    "\n",
    "    \n",
    "#calculate_sequence_probability(all_tag_seq[18],observation_likelihood,transition_probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('PPSS', 'VB', 'TO', 'VB'), 1.8299949392340002e-10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_tag_sequences(sequences: List[List[str]], \n",
    "                        observations: pd.DataFrame, \n",
    "                        transitions: pd.DataFrame) -> List[Tuple[Tuple[str, ...], float]]:\n",
    "    \"\"\"\n",
    "    Calculate the sequences probability for every entry in an array-like object of POS sequences\n",
    "    \n",
    "    :param sequences: array-like object of POS tag sequences\n",
    "    :param observations: array-like object of observation probabilities\n",
    "    :param transitions: array-like object of transition probabilities\n",
    "    :return: array-like object that contains a POS sequence and a probability for every entry\n",
    "    \"\"\"\n",
    "    all_tag_pro=[]\n",
    "    for i in sequences:\n",
    "        line=[]\n",
    "        pro=calculate_sequence_probability(i,observation_likelihood,transition_probability)\n",
    "        line.append(i)\n",
    "        line.append(pro)\n",
    "        all_tag_pro.append(line)\n",
    "        \n",
    "    return all_tag_pro\n",
    "\n",
    "\n",
    "def find_sequence_with_highest_probability(\n",
    "    scored_sequences: List[Tuple[Tuple[str, ...], float]]) -> Tuple[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Returns the POS tag sequence and probability with the highest probability from an array-like object that contains a POS sequence and a probability for every entry\n",
    "    \n",
    "    :param scored_sequences: array-like object that contains a POS sequence and a probability for every entry\n",
    "    :return: Tuple of sequence and probability \n",
    "    \"\"\"\n",
    "    best=0\n",
    "    sent=[]\n",
    "    for i in scored_sequences:\n",
    "        if i[1]>=best:\n",
    "            best=i[1]\n",
    "            sent=i[0]\n",
    "    return (sent,best)\n",
    "\n",
    "scored_sequences=score_tag_sequences(all_tag_seq,observation_likelihood,transition_probability) \n",
    "find_sequence_with_highest_probability(scored_sequences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Why would the naive solution of scoring each tag sequence not scale up to real world problems ? How does the Viterbi algorithm relate to that ? Discuss in up to 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The naive solution essentially is a brute force search with high complexicity $O(s^m)$, where $m$ is the length of the input and $s$ is the number of states in the model. In the contrast, the complexicity of Viterbi algorithm is $O(ms^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1.2 - 6 points</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Viterbi algorithm as shown in slide 23 of our Exercise Session 3 and apply it to the data from above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute the initialization step of Viterbi given transition and observation probability and print the probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.      0.      0.      0.     ]\n",
      " [0.02479 0.      0.      0.     ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "T=len(observation_likelihood.columns)-1\n",
    "N=len(transition_probability)-1\n",
    "viterbi=np.zeros([N,T])\n",
    "backpointer=np.empty([N,T])\n",
    "for j in range(N):\n",
    "    viterbi[j,0]=transition_probability.iloc[0][j+1]*observation_likelihood.iloc[j][1]\n",
    "print(viterbi)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compute the recursion step of Viterbi given transition and observation probability and print the probability matrix again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 5.30258100e-05 0.00000000e+00 1.82999494e-10]\n",
      " [0.00000000e+00 0.00000000e+00 1.83734432e-06 0.00000000e+00]\n",
      " [0.00000000e+00 1.60639200e-09 0.00000000e+00 4.92224542e-13]\n",
      " [2.47900000e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "compare=np.empty(4)\n",
    "\n",
    "for t in range(2,T+1):\n",
    "    #print(\"t=\",t)\n",
    "    for s in range(N):\n",
    "        #print(\"s=\",s)\n",
    "        for i in range(N):\n",
    "            #print(\"i=\",i)\n",
    "            compare[i]=viterbi[i,t-2]*transition_probability.iloc[i+1][s+1]*observation_likelihood.iloc[s][t]\n",
    "            #print(viterbi[i,t-2],transition_probability.iloc[i+1][s+1],observation_likelihood.iloc[s][t])\n",
    "        #print(\"compare:\",compare)\n",
    "        viterbi[s,t-1]=np.max(compare)\n",
    "        \n",
    "print(viterbi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute the termination step of Viterbi given transition and observation probability. Afterwards, print the most likely tag sequence and its probability like you did in 1.1 a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PPSS', 'VB', 'TO', 'VB'), 1.8299949392340002e-10]\n"
     ]
    }
   ],
   "source": [
    "best_path=[]\n",
    "seq=[]\n",
    "loc=np.argmax(viterbi,axis=0)\n",
    "prob=np.max(viterbi[:,N-1])\n",
    "for i in loc:\n",
    "    best_path.append(observation_likelihood.iloc[i][0])\n",
    "seq.append(tuple(best_path))\n",
    "seq.append(prob)\n",
    "print(seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 2: Vectorizing and ML with scikit-learn - 10 Points </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the task of categorizing text data into a set of predefined labels. The most important part of text classification is feature engineering: the process of extracting features from raw text data for machine learning models. We discussed previously what is a bag of words (bow) and why is it important to use it. In today’s class, we have seen how to transform a raw text into a set of features that can be represented as a matrix or a vector. In this exercise, we will practice feature engineering for text classification with <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>. The goal of this exercise is to explore different features and their representations, train and evaluate different classifiers to automatically identify the sentiment polarity of the movie reviews.<br><br>\n",
    "<b>Data:</b> The dataset provided for this exercise contains 5k movie reviews with positive and negative labels, which were taken from IMDB Dataset. It's saved in the file *IMDB_reviews.csv*, which has two columns separated by ','. The first column contains reviews and the second column contains their sentiment labels (0=negative, 1=positive). <br><br>\n",
    "<b>Note:</b> For this task, you may only use spaCy, scikit-learn, NumPy, Pandas and internal packages from Python. Please follow the instructions as given below and in case of questions use our Discussion forum in Moodle, we don’t answer questions via email.<br><br>\n",
    "Please use comments where appropriate to help tutors understand your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Task 2.1 - 5 Points </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Read the data from *IMDB_reviews.csv*. Shuffle and split it into training (60%), development (20%) and test(20%) sets. Print the size of the training, development and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first and only time I saw the woman in bla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just don't bother. I thought I would see a mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie is just bad. Bad. Bad. Bad. Now tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First, This movie was made in 1978. So that te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You know all those letters to \"Father Christma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Deliverance is the fascinating, haunting and s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>I saw this one at Sundance, and I can't figure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>I watched this movie knowing that it would be ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Having enjoyed Koyaanisqatsi and Powaqatsi I w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Telly Savalas hams it up as the Mexican revolu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     The first and only time I saw the woman in bla...      1\n",
       "1     Just don't bother. I thought I would see a mov...      0\n",
       "2     This movie is just bad. Bad. Bad. Bad. Now tha...      0\n",
       "3     First, This movie was made in 1978. So that te...      0\n",
       "4     You know all those letters to \"Father Christma...      0\n",
       "...                                                 ...    ...\n",
       "4995  Deliverance is the fascinating, haunting and s...      1\n",
       "4996  I saw this one at Sundance, and I can't figure...      0\n",
       "4997  I watched this movie knowing that it would be ...      0\n",
       "4998  Having enjoyed Koyaanisqatsi and Powaqatsi I w...      0\n",
       "4999  Telly Savalas hams it up as the Mexican revolu...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the training: set: 3000 \n",
      "\n",
      "the size of the development set: 1000 \n",
      "\n",
      "the size of the test set: 1000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"IMDB_reviews.csv\", sep=\",\")\n",
    "display(data)\n",
    "                   \n",
    "text_train, text_dev_test, label_train, label_dev_test = train_test_split(data['text'], data['label'], \n",
    "                                                                  test_size=0.40, \n",
    "                                                                  random_state=42, shuffle=True)#6：4 get train data and rest data.\n",
    "\n",
    "text_dev, text_test, label_dev, label_test = train_test_split(text_dev_test, label_dev_test, \n",
    "                                                                  test_size=0.50, \n",
    "                                                                  random_state=42, shuffle=True)# 1:1 devide rest data into development and test data.\n",
    "\n",
    "print(\"the size of the training: set:\",len(text_train),'\\n')\n",
    "print(\"the size of the development set:\",len(text_dev),'\\n')\n",
    "print(\"the size of the test set:\",len(text_test),'\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Implement the function `train_valid_cls()`. The function should take the training and development set as well as a vectorizer and a classifier as shown below. The function should then train and validate the given classifier using the data and the vectorizer and print the accuracy on the development set.\n",
    "<br>\n",
    "Using `train_valid_cls()` together with the `CountVectorizer()` and the `MultinomialNB()` provided by scikit-learn, train and evaluate two multinomial Naive Bayes classifiers with the training and development sets. One classifier uses the count matrix (absolute occurrence of each word) and the other one uses the binary count matrix (binary, whether a word occurs in a text) as features. Print the accuracy of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def train_valid_cls(train_texts: pd.Series, train_labels: pd.Series, dev_texts: pd.Series, dev_labels: pd.Series, \n",
    "                    vectorizer: CountVectorizer, classifier: MultinomialNB) -> None:\n",
    "    \"\"\"\n",
    "    Train and validate the classifier with the given data and vectorizer, print the accuracy of \n",
    "    the trained classifier on the development set.\n",
    "    \n",
    "    @param train_texts: array-like object containing review texts from the training set\n",
    "    @param train_labels: array-like object with corresponding sentiment labels for train_texts\n",
    "    @param dev_texts: array-like object containing review texts from the development set\n",
    "    @param dev_labels: array-like object with corresponding sentiment labels for dev_texts\n",
    "    @param vectorizer: a customized scikit-learn Vectorizer \n",
    "    @param classifier: a scikit-learn Classifier\n",
    "    \"\"\"\n",
    "    train_texts_counts=vectorizer.fit_transform(train_texts) # vectorize the train data\n",
    "    dev_texts_counts=vectorizer.transform(text_dev)# vectorize the development data\n",
    "    classifier.fit(train_texts_counts,train_labels)# train the model\n",
    "    pred=classifier.predict(dev_texts_counts)    # use model to predict the development data\n",
    "    print(\"Acc:\",metrics.accuracy_score(dev_labels,pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.819\n",
      "Acc: 0.828\n"
     ]
    }
   ],
   "source": [
    "train_valid_cls(text_train,label_train, text_dev, label_dev, CountVectorizer(lowercase=True), MultinomialNB())\n",
    "train_valid_cls(text_train,label_train, text_dev, label_dev, CountVectorizer(lowercase=True,binary=True), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Using the function `train_valid_cls()` from b), explore at least 3 different ranges of n-grams (introduce bigram, trigram ... features) and try to find the best one for training the multinomial Naive Bayes classifier with count matrix or binary count matrix. Report the accuracy on the development set for every range you tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on count matrix under the grams  (2, 2)\n",
      "Acc: 0.849\n",
      "The accuracy on binary count under the grams  (2, 2)\n",
      "Acc: 0.852\n",
      "The accuracy on count matrix under the grams  (3, 3)\n",
      "Acc: 0.819\n",
      "The accuracy on binary count under the grams  (3, 3)\n",
      "Acc: 0.821\n",
      "The accuracy on count matrix under the grams  (4, 4)\n",
      "Acc: 0.741\n",
      "The accuracy on binary count under the grams  (4, 4)\n",
      "Acc: 0.744\n"
     ]
    }
   ],
   "source": [
    "n_gram=[(2,2),(3,3),(4,4)]\n",
    "for gram in n_gram:\n",
    "    print(\"The accuracy on count matrix under the grams \",gram)\n",
    "    train_valid_cls(text_train,label_train, text_dev, label_dev, CountVectorizer(lowercase=True,ngram_range=gram), MultinomialNB())# ngram_range(a,b) means that from a to b as a group to vectorize  \n",
    "    print(\"The accuracy on binary count under the grams \",gram)\n",
    "    train_valid_cls(text_train,label_train, text_dev, label_dev, CountVectorizer(lowercase=True,ngram_range=gram,binary=True), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** As the results shown above, the best perfamance is bigrams. the accuarcies for both matrix ,count matrix and binary count matrix, are the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Task 2.2 - 5 Points </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Tokenize every review text in the training and development sets using spaCy (It may take a few minutes). You should store the tokens as spaCy token objects (instead of strings) for the subsequent tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "train_text_doc=[]\n",
    "dev_text_doc=[]\n",
    "for tx in text_train:\n",
    "    train_text_doc.append(nlp(tx))# tokenize every review text in the training \n",
    "for tx in text_dev:\n",
    "    dev_text_doc.append(nlp(tx))# tokenize every review text in the development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Extract the spaCy word vectors of the tokens from a). For each review text, calculate the average of all word vectors as its vector representation. Then train a gaussian Naive Bayes classifier (sklearn.naive_bayes.GaussianNB) with the obtained vector representations, evaluate and print its accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "train_vector=[]\n",
    "dev_vector=[]\n",
    "for review in train_text_doc:\n",
    "    train_vector.append(review.vector)  # calculate the average of all word vectors \n",
    "    \n",
    "for review in dev_text_doc:\n",
    "    dev_vector.append(review.vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.619\n"
     ]
    }
   ],
   "source": [
    "classifier_G=GaussianNB()\n",
    "classifier_G.fit(train_vector,label_train)\n",
    "pred=classifier_G.predict(dev_vector)\n",
    "print(\"Acc:\",metrics.accuracy_score(label_dev,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Now, filter out all tokens except verbs, adjectives and adverbs from the tokenized review texts from a). You should store the remaining tokens as spaCy token objects (instead of strings). \n",
    "Extract the spaCy word vectors of the remaining tokens and create a vector representation of each review text like you did in b). Then you should again train and evaluate a GaussianNB classifier and print its accuracy on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.589\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_attr1 = [{'POS': 'ADJ'}]\n",
    "pattern_attr2 = [{'POS': 'VERB'}]\n",
    "pattern_attr3 = [{'POS': 'ADV'}]\n",
    "matcher.add('PATTERN_ATTR', [pattern_attr1])\n",
    "matcher.add('PATTERN_ATTR', [pattern_attr2])\n",
    "matcher.add('PATTERN_ATTR', [pattern_attr3])\n",
    "\n",
    "train_token=[]\n",
    "for review in train_text_doc:\n",
    "    matches=matcher(review) # only get verbs, adjectives and adverbs from every tokenized review text\n",
    "    tokens=[]\n",
    "    for match_id, start, end in matches:\n",
    "        tokens.append(review[start:end].text) # store the data that have been filted out\n",
    "    train_token.append(nlp(str(tokens)))\n",
    "\n",
    "dev_token=[]\n",
    "for review in dev_text_doc:\n",
    "    matches=matcher(review)\n",
    "    tokens=[]\n",
    "    for match_id, start, end in matches:\n",
    "        tokens.append(review[start:end].text)\n",
    "    dev_token.append(nlp(str(tokens)))\n",
    "\n",
    "train_token_vector=[]\n",
    "dev_token_vector=[]\n",
    "for review in train_token:\n",
    "    train_token_vector.append(review.vector)  # vectorize the filted data      \n",
    "for review in dev_token:\n",
    "    dev_token_vector.append(review.vector)\n",
    "\n",
    "classifier_G.fit(train_token_vector,label_train)\n",
    "pred=classifier_G.predict(dev_token_vector)\n",
    "print(\"Acc:\",metrics.accuracy_score(label_dev,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Choose your best model from Task 2.1 and Task 2.2 and test it on the test set, print the accuracy. Why is it important to evaluate your final model on a previously unused test set? Explain it in up to 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams for multinomial Naive Bayes classifier with count binary:\n",
      "Acc: 0.493\n",
      "calculate the average of all word vectors and train a gaussian Naive Bayes classifier:\n",
      "Acc: 0.638\n"
     ]
    }
   ],
   "source": [
    "test_text_doc=[]\n",
    "test_vector=[]\n",
    "print(\"bigrams for multinomial Naive Bayes classifier with count binary:\")\n",
    "train_valid_cls(text_train,label_train, text_test, label_test, CountVectorizer(lowercase=True,ngram_range=(2,2),binary=True), MultinomialNB())\n",
    "print(\"calculate the average of all word vectors and train a gaussian Naive Bayes classifier:\")\n",
    "for tx in text_test:\n",
    "    test_text_doc.append(nlp(tx))\n",
    "for review in test_text_doc:\n",
    "    test_vector.append(review.vector)\n",
    "classifier_G.fit(train_vector,label_train)\n",
    "pred=classifier_G.predict(test_vector)\n",
    "print(\"Acc:\",metrics.accuracy_score(label_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please upload in Moodle your working Jupyter-Notebook <b>before next the lab session</b> <span style=\"color:red\">(Dec 9th, 16:14)</span>. Submission format: Group_No_Exercise_No.zip<br>\n",
    "Submission should contain your filled out Jupyter notebook template (naming schema: Group_No_Exercise_No.ipynb) and any auxiliar files that are necessary to run your code (e.g. datasets provided by us).<br>\n",
    "Each submission must only be handed in once per group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
